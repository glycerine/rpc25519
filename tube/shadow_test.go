package tube

import (
	"bytes"
	"context"
	"fmt"
	"strings"
	//"sort"
	"time"

	//rpc "github.com/glycerine/rpc25519"
	"testing"
)

var _ = fmt.Sprintf

// tubeadd -shadow or calling AddPeerIDToCluster with nonVoting=true
// should add to s.state.ShadowReplicas (only if not regular member!),
// and then should get all wal logging.
func Test065_shadow_replicas_get_wal_even_with_leader_change(t *testing.T) {

	onlyBubbled(t, func() {

		ctx := context.Background()
		minClusterSz := 3
		maxClusterSz := 3
		for numNodes := minClusterSz; numNodes <= maxClusterSz; numNodes++ {

			forceLeader := 0
			cfg := NewTubeConfigTest(numNodes, t.Name(), faketime)
			cfg.NoLogCompaction = true // does get applied to memwal now.
			cfg.RpcCfg.QuietTestMode = false
			// why is NoInitialLeaderTimeout?? forceLeader>=0 causes it.

			c, leader, leadi, maxterm := setupTestClusterWithCustomConfig(cfg, t, numNodes, forceLeader, 55)

			_, _, _ = leader, leadi, maxterm

			vv("current replicas cannot instantly become shadow replicas; must remove from membership first")

			forceChange := false
			nonVoting := true
			// same call that tube/tubecli.go does to join cluster.
			node2 := c.Nodes[2]
			baseServerHostPort := node2.BaseServerHostPort()
			errWriteDur := time.Second * 10
			actualLeaderURL := c.Nodes[0].URL

			ctx5sec, canc5 := context.WithTimeout(ctx, 5*time.Second)
			inspAfterAdd, stateSnapshot, err := node2.AddPeerIDToCluster(ctx5sec, forceChange, nonVoting, node2.name, node2.PeerID, node2.PeerServiceName, baseServerHostPort, actualLeaderURL, errWriteDur)
			canc5()
			_ = inspAfterAdd
			_ = stateSnapshot

			if err == nil {
				panic("wanted error: disallow member -> shadow")
			}
			errs := err.Error()
			//vv("errs = '%v'", errs)
			if !strings.Contains(errs, "node is already in replica membership MC, so cannot use ADD_SHADOW_NON_VOTING to add them to ShadowReplica") {
				panic("wanted error: disallow member -> shadow")
			}
			vv("good: disallowed current replica -> shadow.")

			vv("remove node2 from cluster first then try again")

			//const forceChange = false
			nonVoting = false

			inspAfterRemove, stateSnapshot, err := node2.RemovePeerIDFromCluster(ctx, forceChange, nonVoting, node2.name, node2.PeerID, node2.PeerServiceName, baseServerHostPort, actualLeaderURL, errWriteDur)
			panicOn(err)
			_, node2replica := inspAfterRemove.CktReplicaByName[node2.name]
			if node2replica {
				panic("arg. node 2 is still in CktReplica")
			}
			_, node2member := inspAfterRemove.MC.PeerNames.get2(node2.name)
			if node2member {
				panic("arg. node 2 is still in MC")
			}
			vv("good: node 2 has been removed from MC, is no longer TUBE_REPLICA")

			vv("next, add node 2 back but as a shadow replica...")
			ctx5sec, canc5 = context.WithTimeout(ctx, 5*time.Second)

			// works on lead! but not on node2??
			//inspAfterAdd, stateSnapshot, err = c.Nodes[leadi].AddPeerIDToCluster(ctx5sec, nonVoting, node2.name, node2.PeerID, node2.PeerServiceName, baseServerHostPort, actualLeaderURL, errWriteDur)

			// node2 should give an error if it cannot add itself as shadow.
			nonVoting = true
			inspAfterAdd, stateSnapshot, err = node2.AddPeerIDToCluster(ctx5sec, forceChange, nonVoting, node2.name, node2.PeerID, node2.PeerServiceName, baseServerHostPort, actualLeaderURL, errWriteDur)
			canc5()
			panicOn(err)

			_, node2member = inspAfterAdd.ShadowReplicas.PeerNames.get2(node2.name)
			if !node2member {
				panicf("arg. node 2 was not added to ShadowReplicas; state=%v", stateSnapshot)
			}

			_, node2member = inspAfterAdd.MC.PeerNames.get2(node2.name)
			if node2member {
				panic("arg. node 2 got added to MC as well as ShadowReplicas!")
			}

			vv("good: no error this time when adding node2 as non-voting shadow replica. do some WRITEs and verify that the shadow sees them too.")

			var v []byte
			N := 10
			for i := range N {

				// Write
				v = []byte(fmt.Sprintf("%v", i))
				_, err := c.Nodes[0].Write(bkg, "", "a", v, 0, nil)
				panicOn(err)

				vv("good, past Write at i = %v", i)

				// since shadows are not in quorum, they
				// can lag behind, give it a sec to replicate
				time.Sleep(time.Second)

				// Read all member nodes, including the shadow.
				var lli0 int64 = -1
				for j := range numNodes {

					look := c.Nodes[j].Inspect()
					if j == 0 {
						lli0 = look.LastLogIndex
					} else {
						llij := look.LastLogIndex
						if llij != lli0 {
							panicf("node %v is behind! has lli=%v, while leader is at %v", j, llij, lli0)
						}
					}

					vv("good, we match lli=%v on i=%v, at node j=%v", lli0, i, j)
				}
			}
			// we thought above was checking... but apparently not!
			// because: remember now that Reads get forwarded to the leader!
			vv("good: saw N=%v writes also to shadow", N)

			look := c.Nodes[2].Inspect()
			val, err := look.State.KVStoreRead("", "a")
			panicOn(err)

			if !bytes.Equal(v, val) {
				panicf("expected last values of 'a' on node2 to be '%v'; got '%v'", string(v), string(val))
			}

			vv("change leader, then do more writes and verify shadow gets them")

			vv("power off leader, then sleep 10 sec")

			c.Nodes[leadi].SimCrash()

			time.Sleep(time.Second * 10)

			//snap := c.SimnetSnapshot()
			//vv("5 seconds after crashing the leader") // , snap = '%v'", snap.LongString())

			vv("done with 10 sec of sleep, power ON node_0, then sleep 20 sec")

			// try to get main i loop going on rebooted node_0!
			c.Nodes[0].cfg.testCluster.NoInitialLeaderTimeout = false
			c.SimBoot(leadi)
			time.Sleep(time.Second * 20)

			vv("done with 20 sec of sleep after node_0 back on.")

			// creates data races, watch out under -race.
			// e.g. tube.go:2903 vs tube.go:6541
			//vv("node_0 is %v", c.Nodes[0].me())
			//vv("node_1 is %v", c.Nodes[1].me())
			//vv("node_2 is %v", c.Nodes[2].me())

			// actually either node_0 or node_1
			// can become leader, since node_2 is shadow
			// and does not participate in voting; hence
			// while node_0 was offline, there will have
			// been no leader available at all!

			//time.Sleep(time.Second * 10)

			snap := c.SimnetSnapshot()
			vv("snap = '%v'", snap) // .LongString())

			haveLeader := false
			newLeadi := -1
			for j := numNodes - 1; j >= 0; j-- {
				look := c.Nodes[j].Inspect()
				if look.Role == LEADER {
					haveLeader = true
					newLeadi = j
				}
				// ah problem: new leaders are not establishing
				// connections to the ShadowReplicas? or
				// node_0 the rejoined follower is not connected
				// to the shadow replica node_2... yeah, that
				// might be okay! but no. I think it is messing with elections;
				// the lack of restored circuits.
				if true {
					n := len(look.CktAllByName)
					vv("node %v has CktAllByName: '%v'\n\n node='%v'", j, look.CktAllByName, c.Nodes[j].me())

					vv("j=%v node has ckt count %v to: '%v'", j, n, keys(look.CktAllByName))
					if n != numNodes {
						vv("bad! will panic! node j=%v had %v CktAllByName, not numNodes=%v", j, n, numNodes)
						//snap := c.SimnetSnapshot()
						//vv("pre-panic snap = '%v'", snap)
						panic("fix the above lack of CktAllByName entries!")
					}
				}
			}
			if !haveLeader {
				panic("should have elected leader")
			}

			vv("cluster recovered from leader restart, try writes and check shadow again")

			for i := N; i < 2*N; i++ {

				// Write
				v = []byte(fmt.Sprintf("%v", i))
				_, err := c.Nodes[0].Write(bkg, "", "a", v, 0, nil)
				panicOn(err)

				vv("good, past Write at i = %v", i)

				// Read all member nodes, including the shadow.

				// give the shadow a little time to catch up.
				time.Sleep(time.Second)

				var lli0 int64 = -1
				for k := range numNodes {
					// arrange to read the leader first,
					// even though not 0 any more
					j := (newLeadi + k) % numNodes
					look := c.Nodes[j].Inspect()
					if k == 0 {
						lli0 = look.LastLogIndex
					} else {
						llij := look.LastLogIndex
						if llij != lli0 {
							// ugh! intermittent hit this when
							// shadow was not yet caught up.
							if j == 2 {
								vv("ignoring node_2 our shadow replica getting a little behind... lli=%v vs leader at %v", llij, lli0)
								time.Sleep(time.Second * 2)
							} else {
								panicf("node %v is behind! has lli=%v, while leader is at %v", j, llij, lli0)
							}
						}
					}

					vv("good, we match lli=%v on i=%v, at node j=%v", lli0, i, j)
				}
			}
			vv("good: saw N = %v more writes also to shadow", N)

			vv("all good 065: finish up test by closing down cluster")
			// cleanup
			c.Close()
		} // for numNodes loop
	})
}
